"""Webæœç´¢Agent - åŸºäºReactæ¡†æ¶çš„æ™ºèƒ½ä½“å®ç°ï¼ˆä¿®å¤ç‰ˆï¼‰"""

import asyncio
import json
from typing import Optional, Dict, List, Literal, Any, AsyncGenerator, Callable
from llm_0908 import LLM
from tools.tavily_search import tavily_search
from tools.function_schema import tools

class WebSearchAgent:
    """Webæœç´¢æ™ºèƒ½ä½“ - æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨å’Œæ™ºèƒ½å†³ç­–
    
    æ ¸å¿ƒç‰¹æ€§:
    - Reactæ¡†æ¶ï¼šThink -> Act -> Observe å¾ªç¯
    - å†…ç½®è®°å¿†ç®¡ç†ï¼šç»´æŠ¤å®Œæ•´å¯¹è¯å†å²
    - æ™ºèƒ½å·¥å…·è°ƒç”¨ï¼šè‡ªä¸»å†³å®šä½•æ—¶ä½¿ç”¨å·¥å…·
    - æµå¼/éæµå¼ï¼šæ”¯æŒä¸¤ç§è¾“å‡ºæ¨¡å¼
    """
    
    def __init__(self,
                 api_key: str,
                 base_url: str = "https://api.deepseek.com/v1",
                 model: str = "deepseek-chat",
                 tool_choice: Literal["auto", "required", "none"] = "auto",
                 max_steps: int = 5,
                 temperature: float = 0.7,
                 max_tokens: int = 4096):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“
        
        Args:
            max_steps: æœ€å¤§æ‰§è¡Œæ­¥æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
        """
        self.llm = LLM(
            api_key=api_key,
            base_url=base_url,
            model=model,
            tool_choice=tool_choice,
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        self.max_steps = max_steps
        self.current_step = 0
        
        # å·¥å…·æ³¨å†Œè¡¨
        self.tool_functions = {
            "tavily_search": tavily_search
        }
        
        # ç³»ç»Ÿæç¤ºè¯ - å®šä¹‰æ™ºèƒ½ä½“çš„è¡Œä¸ºæ¨¡å¼
        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æœç´¢åŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

## æ ¸å¿ƒèƒ½åŠ›
1. **æ™ºèƒ½åˆ¤æ–­**ï¼šåˆ¤æ–­é—®é¢˜æ˜¯å¦éœ€è¦æœç´¢äº’è”ç½‘ä¿¡æ¯
2. **å¤šè½®æœç´¢**ï¼šå¦‚æœåˆæ¬¡æœç´¢ä¸å¤Ÿå……åˆ†ï¼Œå¯ä»¥è¿›è¡Œå¤šè½®æœç´¢
3. **ç­–ç•¥ä¼˜åŒ–**ï¼šåŸºäºå‰æ¬¡ç»“æœè°ƒæ•´æœç´¢ç­–ç•¥

## å·¥ä½œæµç¨‹
1. **åˆ†æé—®é¢˜**ï¼šç†è§£ç”¨æˆ·æ„å›¾ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦å®æ—¶ä¿¡æ¯
2. **æ‰§è¡Œæœç´¢**ï¼šå¦‚éœ€è¦ï¼Œä½¿ç”¨tavily_searchå·¥å…·è·å–ä¿¡æ¯
3. **è¯„ä¼°ç»“æœ**ï¼šåˆ¤æ–­æœç´¢ç»“æœæ˜¯å¦å……åˆ†å›ç­”é—®é¢˜
4. **è¿­ä»£ä¼˜åŒ–**ï¼šå¦‚ä¸å……åˆ†ï¼Œè°ƒæ•´å…³é”®è¯ç»§ç»­æœç´¢ï¼ˆæœ€å¤š{max_steps}è½®ï¼‰
5. **ç»¼åˆå›ç­”**ï¼šåŸºäºæ‰€æœ‰ä¿¡æ¯æä¾›å‡†ç¡®ã€å…¨é¢çš„ç­”æ¡ˆ

## å†³ç­–åŸåˆ™
- å¦‚æœä½ çš„çŸ¥è¯†åº“èƒ½å……åˆ†å›ç­”ï¼Œç›´æ¥å›ç­”æ— éœ€æœç´¢
- å¦‚æœæ¶‰åŠå®æ—¶ä¿¡æ¯ã€æœ€æ–°åŠ¨æ€ï¼Œç«‹å³æœç´¢
- å¦‚æœæœç´¢ç»“æœå……åˆ†ï¼Œåœæ­¢æœç´¢å¹¶ç»™å‡ºç­”æ¡ˆ
- å¦‚æœæœç´¢ç»“æœä¸è¶³ï¼Œä¼˜åŒ–æœç´¢è¯ç»§ç»­æœç´¢

## é‡è¦æç¤º
- è·å¾—æœç´¢ç»“æœåï¼Œå¿…é¡»åŸºäºç»“æœç”Ÿæˆå®Œæ•´çš„å›ç­”
- ä¸è¦åœ¨æœ€ç»ˆå›ç­”ä¸­è¯´"è®©æˆ‘æœç´¢"æˆ–ç±»ä¼¼çš„è¯
- ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œä¸è¦è¡¨è¾¾ç»§ç»­æœç´¢çš„æ„æ„¿""".format(max_steps=max_steps)
        
        # åˆå§‹åŒ–ç³»ç»Ÿæç¤º
        self._initialize_system_prompt()
    
    def _initialize_system_prompt(self):
        """åˆå§‹åŒ–ç³»ç»Ÿæç¤ºè¯"""
        self.llm.add_message("system", self.system_prompt)
    
    async def think(self, force_answer: bool = False) -> tuple[bool, Optional[str]]:
        """æ€è€ƒé˜¶æ®µï¼šå†³å®šæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
        
        Args:
            force_answer: æ˜¯å¦å¼ºåˆ¶ç”Ÿæˆç­”æ¡ˆï¼ˆä¸ä½¿ç”¨å·¥å…·ï¼‰
        
        Returns:
            (æ˜¯å¦éœ€è¦å·¥å…·, åŠ©æ‰‹å›å¤å†…å®¹)
        """
        # æ„å»ºæç¤º
        next_step_prompt = self._build_next_step_prompt()
        
        # å†³å®šæ˜¯å¦æä¾›å·¥å…·ï¼ˆæœ€åä¸€æ­¥æˆ–å¼ºåˆ¶å›ç­”æ—¶ä¸æä¾›ï¼‰
        should_provide_tools = not force_answer and self.current_step < self.max_steps
        
        # è°ƒç”¨LLM
        response = await self.llm.chat_complete(
            user_input=next_step_prompt if self.current_step > 0 else None,
            tools=tools if should_provide_tools else None,
            tool_functions=self.tool_functions,
            use_history=True,
            verbose=False
        )
        
        # åˆ¤æ–­æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        if hasattr(response, 'tool_calls') and response.tool_calls:
            return True, None
        else:
            content = response if isinstance(response, str) else response.content
            return False, content
    
    async def act(self) -> List[Dict]:
        """è¡ŒåŠ¨é˜¶æ®µï¼šæ‰§è¡Œå·¥å…·è°ƒç”¨"""
        results = []
        
        # è·å–æœ€æ–°çš„åŠ©æ‰‹æ¶ˆæ¯
        last_message = self.llm.get_history()[-1]
        
        if not last_message.get("tool_calls"):
            return results
        
        # æ‰§è¡Œæ¯ä¸ªå·¥å…·è°ƒç”¨
        for tool_call in last_message["tool_calls"]:
            tool_name = tool_call["function"]["name"]
            tool_args = json.loads(tool_call["function"]["arguments"])
            
            print(f"ğŸ”§ æ‰§è¡Œå·¥å…·: {tool_name}")
            print(f"   å‚æ•°: {tool_args}")
            
            try:
                if asyncio.iscoroutinefunction(self.tool_functions[tool_name]):
                    result = await self.tool_functions[tool_name](**tool_args)
                else:
                    result = self.tool_functions[tool_name](**tool_args)
                
                # æ·»åŠ å·¥å…·ç»“æœåˆ°å†å²
                self.llm.add_message("tool", str(result), tool_call_id=tool_call["id"])
                
                results.append({
                    "tool": tool_name,
                    "result": str(result)[:200] + "..." if len(str(result)) > 200 else str(result)
                })
                
            except Exception as e:
                error_msg = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
                self.llm.add_message("tool", error_msg, tool_call_id=tool_call["id"])
                results.append({"tool": tool_name, "error": error_msg})
        
        return results
    
    def _build_next_step_prompt(self) -> str:
        """æ„å»ºå¼•å¯¼æç¤ºè¯"""
        if self.current_step == 0:
            return ""
        elif self.current_step < self.max_steps:
            return "åŸºäºæœç´¢ç»“æœï¼Œè¯·ç›´æ¥ç»™å‡ºå®Œæ•´çš„ç­”æ¡ˆã€‚å¦‚æœä¿¡æ¯ä¸è¶³å¯ä»¥ç»§ç»­æœç´¢ï¼Œä½†ä¼˜å…ˆè€ƒè™‘ç›´æ¥å›ç­”ã€‚"
        else:
            return "æ€»ç»“ä»¥ä¸Šæ‰€æœ‰ä¿¡æ¯ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ"
    
    def run(self, user_input: str, stream: bool = False):
        """è¿è¡Œæ™ºèƒ½ä½“"""
        if stream:
            return self._run_stream(user_input)
        else:
            return self._run_complete(user_input)
    
    async def _run_complete(self, user_input: str) -> str:
        """éæµå¼è¿è¡Œ"""
        self.current_step = 0
        
        # æ·»åŠ ç”¨æˆ·è¾“å…¥
        self.llm.add_message("user", user_input)
        
        print(f"\nğŸ¤” æ™ºèƒ½ä½“å¼€å§‹å¤„ç†: {user_input}\n")
        
        while self.current_step < self.max_steps:
            self.current_step += 1
            print(f"\nğŸ“ æ­¥éª¤ {self.current_step}/{self.max_steps}")
            
            # Think: æ€è€ƒæ˜¯å¦éœ€è¦å·¥å…·
            needs_tool, content = await self.think()
            
            if needs_tool:
                # Act: æ‰§è¡Œå·¥å…·
                print("ğŸ’­ å†³å®šä½¿ç”¨å·¥å…·...")
                tool_results = await self.act()
                
                if tool_results:
                    for result in tool_results:
                        if "error" in result:
                            print(f"   âŒ {result['tool']}: {result['error']}")
                        else:
                            print(f"   âœ… {result['tool']}: è·å–ç»“æœæˆåŠŸ")
                    
                    # å·¥å…·æ‰§è¡Œåï¼Œéœ€è¦å†æ¬¡è°ƒç”¨LLMç”ŸæˆåŸºäºç»“æœçš„å›ç­”
                    if self.current_step == self.max_steps:
                        # å¦‚æœæ˜¯æœ€åä¸€æ­¥ï¼Œå¼ºåˆ¶ç”Ÿæˆç­”æ¡ˆ
                        _, final_answer = await self.think(force_answer=True)
                        if final_answer:
                            print(f"\nâœ¨ æ™ºèƒ½ä½“å®Œæˆæ€è€ƒ")
                            return final_answer
                    continue
            
            # å¦‚æœæœ‰å†…å®¹è¿”å›ï¼Œè¯´æ˜æ™ºèƒ½ä½“è®¤ä¸ºå¯ä»¥å›ç­”äº†
            if content:
                print(f"\nâœ¨ æ™ºèƒ½ä½“å®Œæˆæ€è€ƒ")
                return content
        
        # è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œå¼ºåˆ¶ç”Ÿæˆç­”æ¡ˆ
        print(f"\nâš ï¸ è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
        _, final_answer = await self.think(force_answer=True)
        return final_answer or "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è·å–è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
    
    async def _run_stream(self, user_input: str) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼è¿è¡Œ"""
        self.current_step = 0
        
        # æ·»åŠ ç”¨æˆ·è¾“å…¥
        self.llm.add_message("user", user_input)
        
        yield {"type": "start", "data": f"å¼€å§‹å¤„ç†: {user_input}"}
        
        while self.current_step < self.max_steps:
            self.current_step += 1
            yield {"type": "step", "data": f"æ­¥éª¤ {self.current_step}/{self.max_steps}"}
            
            # æµå¼æ€è€ƒ
            has_tool_calls = False
            collected_content = []
            
            # å†³å®šæ˜¯å¦æä¾›å·¥å…·
            should_provide_tools = self.current_step < self.max_steps
            
            # ä¿®å¤ï¼šç¬¬ä¸€æ­¥ä¸éœ€è¦é¢å¤–æç¤º
            extra_prompt = self._build_next_step_prompt() if self.current_step > 1 else None
            
            async for chunk in self.llm.chat_stream(
                user_input=extra_prompt,
                tools=tools if should_provide_tools else None,
                tool_functions=self.tool_functions,
                use_history=True
            ):
                if chunk["type"] == "content":
                    collected_content.append(chunk["data"])
                    yield chunk
                elif chunk["type"] == "tool_executing":
                    has_tool_calls = True
                    yield chunk
                elif chunk["type"] == "tool_result":
                    yield chunk
                elif chunk["type"] == "done":
                    # å¦‚æœæ‰§è¡Œäº†å·¥å…·ï¼Œéœ€è¦ç»§ç»­ä¸‹ä¸€è½®
                    if has_tool_calls:
                        # å¦‚æœæ˜¯æœ€åä¸€æ­¥ï¼Œå¼ºåˆ¶ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
                        if self.current_step == self.max_steps:
                            yield {"type": "generating_answer", "data": "ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ..."}
                            async for chunk in self.llm.chat_stream(
                                user_input="åŸºäºæ‰€æœ‰æœç´¢ç»“æœï¼Œè¯·ç»™å‡ºå®Œæ•´çš„ç­”æ¡ˆã€‚",
                                tools=None,  # ä¸æä¾›å·¥å…·
                                tool_functions=None,
                                use_history=True
                            ):
                                if chunk["type"] == "content":
                                    yield chunk
                                elif chunk["type"] == "done":
                                    yield {"type": "complete", "data": "å®Œæˆ"}
                                    return
                    elif collected_content:
                        # æ²¡æœ‰å·¥å…·è°ƒç”¨ä¸”æœ‰å†…å®¹ï¼Œè¯´æ˜å®Œæˆ
                        yield {"type": "complete", "data": "".join(collected_content)}
                        return
        
        yield {"type": "max_steps_reached", "data": "è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶"}
    
    def reset(self):
        """é‡ç½®æ™ºèƒ½ä½“çŠ¶æ€"""
        self.llm.clear_history()
        self.current_step = 0
        self._initialize_system_prompt()
        print("âœ¨ æ™ºèƒ½ä½“å·²é‡ç½®\n")
    
    def get_history(self) -> List[Dict]:
        """è·å–å¯¹è¯å†å²"""
        return self.llm.get_history()
    
    def set_max_steps(self, max_steps: int):
        """è®¾ç½®æœ€å¤§æ­¥æ•°"""
        self.max_steps = max_steps
        print(f"ğŸ“Š æœ€å¤§æ­¥æ•°è®¾ç½®ä¸º: {max_steps}\n")

# æµ‹è¯•ä»£ç 
async def test_agent():
    """æµ‹è¯•é‡æ„åçš„æ™ºèƒ½ä½“"""
    
    agent = WebSearchAgent(
        api_key="sk-f5889d58c6db4dd38ca78389a6c7a7e8",
        max_steps=3
    )
    
    # æµ‹è¯•1ï¼šéæµå¼
    print("="*60)
    print("æµ‹è¯•1: éæµå¼æ¨¡å¼")
    print("="*60)
    
    result = await agent.run("2024å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–è·å¾—è€…æ˜¯è°ï¼Ÿ")
    print(f"\næœ€ç»ˆç­”æ¡ˆ:\n{result}")
    
    # é‡ç½®
    agent.reset()
    
    # æµ‹è¯•2ï¼šæµå¼
    print("\n" + "="*60)
    print("æµ‹è¯•2: æµå¼æ¨¡å¼")
    print("="*60)
    
    async for chunk in agent.run("ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", stream=True):
        if chunk["type"] == "content":
            print(chunk["data"], end="", flush=True)
        elif chunk["type"] == "step":
            print(f"\n[{chunk['data']}]")
        elif chunk["type"] == "tool_executing":
            print(f"\nğŸ”§ {chunk['data']}")
        elif chunk["type"] == "complete":
            print(f"\nâœ… å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(test_agent())