"""Webæœç´¢Agentä¼šè¯å±‚ - æ”¯æŒçœŸæ­£çš„æµå¼ä¼ è¾“"""

import asyncio
import json
from typing import Optional, Dict, List, Literal, Any, AsyncGenerator
from enum import Enum
from llm_0908 import LLM
from tools.tavily_search import tavily_search
from tools.function_schema import tools

class ToolMode(Enum):
   """å·¥å…·è°ƒç”¨æ¨¡å¼"""
   NEVER = "never"          # ä»ä¸è°ƒç”¨
   AUTO = "auto"            # è‡ªåŠ¨å†³å®š
   ALWAYS = "always"        # å§‹ç»ˆè°ƒç”¨

# è¯¥æšä¸¾ç±»ToolModeç”¨äºå®šä¹‰Webæœç´¢Agentçš„å·¥å…·è°ƒç”¨æ¨¡å¼ï¼š
# NEVER  è¡¨ç¤ºä»ä¸è°ƒç”¨å·¥å…·ï¼Œå§‹ç»ˆåªç”¨å¤§æ¨¡å‹å¯¹è¯ï¼›
# AUTO   è¡¨ç¤ºç”±å¤§æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·ï¼ˆå¦‚é‡åˆ°éœ€è¦å®æ—¶ä¿¡æ¯æ—¶ï¼‰ï¼›
# ALWAYS è¡¨ç¤ºæ¯æ¬¡å¯¹è¯éƒ½å¼ºåˆ¶è°ƒç”¨å·¥å…·ï¼ˆå¦‚æœç´¢ï¼‰å¹¶åŸºäºå·¥å…·ç»“æœä½œç­”ã€‚
# è¿™æ ·å¯ä»¥çµæ´»æ§åˆ¶Agentæ˜¯å¦ä»¥åŠä½•æ—¶è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æœç´¢APIï¼‰ï¼Œä»¥é€‚åº”ä¸åŒçš„ä¸šåŠ¡åœºæ™¯å’Œéœ€æ±‚ã€‚

class WebSearchAgent:
   """Webæœç´¢Agentä¼šè¯ç®¡ç†ç±»"""
   
   def __init__(self,
                api_key: str,
                base_url: str = "https://api.deepseek.com/v1",
                model: str = "deepseek-chat",
                tool_mode: ToolMode = ToolMode.AUTO,
                max_tool_calls: int = 3,
                temperature: float = 0.7,
                max_tokens: int = 4096,
                stream: bool = True):
       """
       Args:
           api_key: DeepSeek APIå¯†é’¥
           base_url: APIåŸºç¡€URL
           model: æ¨¡å‹åç§°
           tool_mode: å·¥å…·è°ƒç”¨æ¨¡å¼
           max_tool_calls: å•æ¬¡å¯¹è¯æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°
           temperature: æ¸©åº¦å‚æ•°
           max_tokens: æœ€å¤§tokens
           stream: æ˜¯å¦æµå¼è¾“å‡º
       """
       # max_tokenså‚æ•°ç”¨äºæ§åˆ¶å¤§æ¨¡å‹æ¯æ¬¡ç”Ÿæˆå›å¤æ—¶çš„æœ€å¤§tokenï¼ˆæ ‡è®°ï¼‰æ•°ã€‚
       # å®ƒå†³å®šäº†æ¨¡å‹è¾“å‡ºå†…å®¹çš„é•¿åº¦ä¸Šé™ï¼Œé˜²æ­¢ç”Ÿæˆè¿‡é•¿çš„å›å¤å¯¼è‡´æ¶ˆè€—è¿‡å¤šèµ„æºæˆ–è¶…å‡ºæ¥å£é™åˆ¶ã€‚
       # ä¾‹å¦‚ï¼Œmax_tokens=4096è¡¨ç¤ºå•æ¬¡å›å¤æœ€å¤šç”Ÿæˆ4096ä¸ªtokenï¼Œè¶…å‡ºéƒ¨åˆ†ä¼šè¢«æˆªæ–­ã€‚
       self.llm = LLM(
           api_key=api_key,
           base_url=base_url,
           model=model,
           temperature=temperature,
           max_tokens=max_tokens,
           stream=stream
       )
       
       self.tool_mode = tool_mode
       self.max_tool_calls = max_tool_calls
       self.tool_call_count = 0  # å½“å‰å¯¹è¯å·¥å…·è°ƒç”¨è®¡æ•°
       
       # å·¥å…·å‡½æ•°æ˜ å°„
       self.tool_functions = {
           "tavily_search": tavily_search
       }
       
       # session_activeå‚æ•°ç”¨äºç®¡ç†å½“å‰ä¼šè¯æ˜¯å¦å¤„äºæ¿€æ´»çŠ¶æ€ï¼ˆå³æ˜¯å¦ç»§ç»­ä¸ç”¨æˆ·äº¤äº’ï¼‰ã€‚
       # å½“session_activeä¸ºTrueæ—¶ï¼ŒAgentä¼šæŒç»­æ¥æ”¶å’Œå¤„ç†ç”¨æˆ·è¾“å…¥ï¼›
       # å¦‚æœè®¾ç½®ä¸ºFalseï¼Œåˆ™ä¼šè¯ç»ˆæ­¢ï¼ŒAgentä¸å†å“åº”æ–°çš„æ¶ˆæ¯ï¼ˆå¦‚ç”¨æˆ·è¾“å…¥/exitå‘½ä»¤æ—¶ï¼‰ã€‚
       self.session_active = True 
       self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥é€šè¿‡æœç´¢å·¥å…·è·å–æœ€æ–°çš„äº’è”ç½‘ä¿¡æ¯ã€‚
å½“ç”¨æˆ·è¯¢é—®éœ€è¦å®æ—¶ä¿¡æ¯çš„é—®é¢˜æ—¶ï¼Œä½ ä¼šä½¿ç”¨æœç´¢å·¥å…·ã€‚
è¯·åŸºäºæœç´¢ç»“æœæä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚"""
       
       # åˆå§‹åŒ–æ—¶æ³¨å…¥ç³»ç»Ÿæç¤º
       if not self.llm.conversation_history:
           self.llm.add_message("system", self.system_prompt)
   
   def reset_session(self):
       """é‡ç½®ä¼šè¯çŠ¶æ€"""
       self.llm.clear_history()
       self.tool_call_count = 0
       # é‡æ–°æ³¨å…¥ç³»ç»Ÿæç¤º
       self.llm.add_message("system", self.system_prompt)
       print("âœ¨ ä¼šè¯å·²é‡ç½®\n")
   
   def set_tool_mode(self, mode: ToolMode):
       """è®¾ç½®å·¥å…·è°ƒç”¨æ¨¡å¼"""
       self.tool_mode = mode
       print(f"ğŸ”§ å·¥å…·æ¨¡å¼è®¾ç½®ä¸º: {mode.value}\n")
   # åœ¨process_message_streamæ–¹æ³•ä¸­ï¼Œä¼šæ ¹æ®self.tool_modeçš„å€¼å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼ˆå¦‚tavily_searchï¼‰ã€‚
   # é€šè¿‡set_tool_modeæ–¹æ³•å¯ä»¥åŠ¨æ€ä¿®æ”¹self.tool_modeï¼Œä»è€ŒçœŸæ­£æ”¹å˜äº†å·¥å…·è°ƒç”¨æ¨¡å¼ã€‚
   
   def set_max_tool_calls(self, max_calls: int):
       """è®¾ç½®æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°"""
       self.max_tool_calls = max_calls
       print(f"ğŸ“Š æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°è®¾ç½®ä¸º: {max_calls}\n")
   
   def set_stream_mode(self, stream: bool):
       """è®¾ç½®æµå¼è¾“å‡ºæ¨¡å¼"""
       self.llm.stream = stream
       mode_str = "æµå¼" if stream else "éæµå¼"
       print(f"ğŸ“¡ è¾“å‡ºæ¨¡å¼è®¾ç½®ä¸º: {mode_str}\n")
   
   async def process_message_stream(self, user_input: str) -> AsyncGenerator[Dict[str, Any], None]:
       """æµå¼å¤„ç†å•æ¡ç”¨æˆ·æ¶ˆæ¯ - è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ä¾›SSEä½¿ç”¨
       
       Args:
           user_input: ç”¨æˆ·è¾“å…¥
           
       Yields:
           Dict: åŒ…å«typeå’Œdataçš„äº‹ä»¶å­—å…¸
       """
       # é‡ç½®å•æ¬¡å¯¹è¯çš„å·¥å…·è°ƒç”¨è®¡æ•°
       self.tool_call_count = 0
       
       # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
       self.llm.add_message("user", user_input)
       
       # æ ¹æ®å·¥å…·æ¨¡å¼å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
       if self.tool_mode == ToolMode.NEVER:
           # ä¸ä½¿ç”¨å·¥å…·ï¼Œç›´æ¥å¯¹è¯
           async for chunk in self._chat_without_tools_stream():
               yield chunk
       elif self.tool_mode == ToolMode.ALWAYS:
           # å§‹ç»ˆä½¿ç”¨å·¥å…·
           async for chunk in self._chat_with_tools_stream(force_tool=True):
               yield chunk
       else:  # AUTOæ¨¡å¼
           # è®©æ¨¡å‹å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
           async for chunk in self._chat_with_tools_stream(force_tool=False):
               yield chunk
   
   async def _chat_without_tools_stream(self) -> AsyncGenerator[Dict[str, Any], None]:
       """ä¸ä½¿ç”¨å·¥å…·çš„æµå¼å¯¹è¯"""
       async for chunk in self.llm.chat_stream():
           if chunk["type"] == "content":
               yield {
                   "type": "assistant_content",
                   "data": chunk["data"]
               }
           elif chunk["type"] == "done":
               yield {
                   "type": "complete",
                   "data": {
                       "final_content": chunk["data"]["content"],
                       "tool_calls": 0
                   }
               }
   
   async def _chat_with_tools_stream(self, force_tool: bool = False) -> AsyncGenerator[Dict[str, Any], None]:
       """ä½¿ç”¨å·¥å…·çš„æµå¼å¯¹è¯"""
       
       # ç‰¹æ®Šæƒ…å†µå¤„ç†ï¼šå·¥å…·è°ƒç”¨æ¬¡æ•°ä¸º0
       if self.max_tool_calls == 0:
           yield {
               "type": "system_message",
               "data": "å·¥å…·è°ƒç”¨æ¬¡æ•°ä¸º0ï¼Œç›´æ¥ç”Ÿæˆå›ç­”..."
           }
           
           async for chunk in self.llm.chat_stream(tools=None):
               if chunk["type"] == "content":
                   yield {
                       "type": "assistant_content",
                       "data": chunk["data"]
                   }
               elif chunk["type"] == "done":
                   yield {
                       "type": "complete",
                       "data": {
                           "final_content": chunk["data"]["content"],
                           "tool_calls": 0
                       }
                   }
           return
       
       # æ­£å¸¸å·¥å…·è°ƒç”¨æµç¨‹
       while self.tool_call_count < self.max_tool_calls:
           # è®¾ç½®å·¥å…·é€‰æ‹©æ¨¡å¼
           if force_tool:
               tool_choice = "required"  # ALWAYS æ¨¡å¼ä¸‹å§‹ç»ˆå¼ºåˆ¶ä½¿ç”¨å·¥å…·
           else:
               tool_choice = "auto"      # AUTO æ¨¡å¼ä¸‹è®©æ¨¡å‹å†³å®š
           
           # æ”¶é›†å·¥å…·è°ƒç”¨
           tool_calls_to_execute = []
           has_content = False
           
           # è°ƒç”¨æ¨¡å‹
           async for chunk in self.llm.chat_stream(tools=tools, tool_choice=tool_choice):
               if chunk["type"] == "content":
                   has_content = True
                   yield {
                       "type": "assistant_content",
                       "data": chunk["data"]
                   }
               elif chunk["type"] == "tool_call_complete":
                   tool_calls_to_execute.append(chunk["data"])
               elif chunk["type"] == "done":
                   # ä¸€è½®å¯¹è¯å®Œæˆ
                   if not tool_calls_to_execute:
                       # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ
                       yield {
                           "type": "complete",
                           "data": {
                               "final_content": chunk["data"]["content"],
                               "tool_calls": self.tool_call_count
                           }
                       }
                       return
                   
                   # æ‰§è¡Œå·¥å…·è°ƒç”¨
                   yield {
                       "type": "tool_execution_start",
                       "data": f"æ‰§è¡Œæœç´¢ (ç¬¬{self.tool_call_count + 1}æ¬¡)"
                   }
                   
                   for tool_call in tool_calls_to_execute:
                       func_name = tool_call["function"]["name"]
                       
                       # å®‰å…¨è§£æJSONå‚æ•°
                       try:
                           func_args = json.loads(tool_call["function"]["arguments"] or "{}")
                       except Exception as e:
                           error_msg = f"å·¥å…·å‚æ•°è§£æå¤±è´¥: {e}"
                           yield {
                               "type": "tool_error",
                               "data": error_msg
                           }
                           self.llm.add_message("tool", error_msg, tool_call_id=tool_call["id"])
                           continue
                       
                       if func_name in self.tool_functions:
                           # æ‰§è¡Œå·¥å…·
                           try:
                               yield {
                                   "type": "tool_executing",
                                   "data": {
                                       "name": func_name,
                                       "query": func_args.get('query', 'N/A')
                                   }
                               }
                               
                               if asyncio.iscoroutinefunction(self.tool_functions[func_name]):
                                   result = await self.tool_functions[func_name](**func_args)
                               else:
                                   result = self.tool_functions[func_name](**func_args)
                               
                               # æ·»åŠ å·¥å…·ç»“æœåˆ°å†å²
                               self.llm.add_message("tool", str(result), tool_call_id=tool_call["id"])
                               
                               yield {
                                   "type": "tool_result",
                                   "data": {
                                       "name": func_name,
                                       "result_preview": str(result)[:200] + "..." if len(str(result)) > 200 else str(result)
                                   }
                               }
                               
                               self.tool_call_count += 1
                               
                           except Exception as e:
                               error_msg = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {e}"
                               yield {
                                   "type": "tool_error",
                                   "data": error_msg
                               }
                               self.llm.add_message("tool", error_msg, tool_call_id=tool_call["id"])
                       else:
                           error_msg = f"æœªçŸ¥å·¥å…·: {func_name}"
                           yield {
                               "type": "tool_error",
                               "data": error_msg
                           }
                           self.llm.add_message("tool", error_msg, tool_call_id=tool_call["id"])
                   
                   # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ä¸Šé™
                   if self.tool_call_count >= self.max_tool_calls:
                       yield {
                           "type": "tool_limit_reached",
                           "data": f"å·²è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°({self.max_tool_calls}æ¬¡)"
                       }
                       
                       # æ·»åŠ ç³»ç»Ÿæç¤º
                       self.llm.add_message(
                           "tool",
                           f"[ç³»ç»Ÿæç¤º] å·²è¾¾åˆ°å·¥å…·è°ƒç”¨ä¸Šé™({self.max_tool_calls}æ¬¡)ï¼Œè¯·åŸºäºç°æœ‰ä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚",
                           tool_call_id="system_limit"
                       )
                       
                       # åŸºäºç°æœ‰ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆå›ç­”
                       yield {
                           "type": "final_answer_start",
                           "data": "åŸºäºæœç´¢ç»“æœç”Ÿæˆå›ç­”..."
                       }
                       
                       async for final_chunk in self.llm.chat_stream(tools=None):
                           if final_chunk["type"] == "content":
                               yield {
                                   "type": "assistant_content",
                                   "data": final_chunk["data"]
                               }
                           elif final_chunk["type"] == "done":
                               yield {
                                   "type": "complete",
                                   "data": {
                                       "final_content": final_chunk["data"]["content"],
                                       "tool_calls": self.tool_call_count
                                   }
                               }
                       return
                   
                   # ç»§ç»­ä¸‹ä¸€è½®ï¼ˆå¦‚æœè¿˜æ²¡è¾¾åˆ°ä¸Šé™ï¼‰
                   break
   
   async def process_message(self, user_input: str) -> str:
       """å¤„ç†å•æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆéæµå¼ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰
       
       Args:
           user_input: ç”¨æˆ·è¾“å…¥
           
       Returns:
           str: Agentå“åº”
       """
       # æ”¶é›†æµå¼è¾“å‡ºçš„å®Œæ•´å†…å®¹
       full_content = []
       
       async for chunk in self.process_message_stream(user_input):
           if chunk["type"] == "assistant_content":
               full_content.append(chunk["data"])
               print(chunk["data"], end="", flush=True)
           elif chunk["type"] == "tool_executing":
               print(f"\n[æ‰§è¡Œæœç´¢]: {chunk['data']['query']}")
           elif chunk["type"] == "tool_result":
               print(f"[æœç´¢ç»“æœ]: {chunk['data']['result_preview'][:100]}...")
           elif chunk["type"] == "final_answer_start":
               print(f"\n[{chunk['data']}]\n", end="")
           elif chunk["type"] == "complete":
               print()  # æ¢è¡Œ
               return chunk["data"]["final_content"] or "".join(full_content)
       
       return "".join(full_content)
   
   async def run_interactive(self):
       """è¿è¡Œäº¤äº’å¼ä¼šè¯"""
       print("="*60)
       print("ğŸ¤– Webæœç´¢Agent - äº¤äº’å¼ä¼šè¯")
       print("="*60)
       print("å‘½ä»¤è¯´æ˜:")
       print("  /reset    - æ¸…ç©ºå¯¹è¯å†å²")
       print("  /mode     - åˆ‡æ¢å·¥å…·è°ƒç”¨æ¨¡å¼")
       print("  /stream   - åˆ‡æ¢æµå¼/éæµå¼è¾“å‡º")
       print("  /max N    - è®¾ç½®æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°(Nä¸ºæ•°å­—)")
       print("  /history  - æŸ¥çœ‹å¯¹è¯å†å²")
       print("  /exit     - é€€å‡ºä¼šè¯")
       print(f"\nå½“å‰è®¾ç½®: å·¥å…·æ¨¡å¼={self.tool_mode.value}, æµå¼={self.llm.stream}, æœ€å¤§è°ƒç”¨={self.max_tool_calls}æ¬¡")
       print("="*60 + "\n")
       
       while self.session_active:
           try:
               # è·å–ç”¨æˆ·è¾“å…¥
               user_input = input("\n[ç”¨æˆ·]: ").strip()
               
               if not user_input:
                   continue
               
               # å¤„ç†å‘½ä»¤
               if user_input.startswith("/"):
                   await self._handle_command(user_input)
                   continue
               
               # å¤„ç†æ™®é€šæ¶ˆæ¯
               print()
               if self.llm.stream:
                   # æµå¼è¾“å‡º
                   print("[åŠ©æ‰‹]: ", end="")
                   async for chunk in self.process_message_stream(user_input):
                       if chunk["type"] == "assistant_content":
                           print(chunk["data"], end="", flush=True)
                       elif chunk["type"] == "tool_executing":
                           print(f"\n[æ‰§è¡Œæœç´¢]: {chunk['data']['query']}")
                       elif chunk["type"] == "final_answer_start":
                           print(f"\n[åŸºäºæœç´¢ç»“æœç”Ÿæˆå›ç­”]: ", end="")
                       elif chunk["type"] == "complete":
                           print()  # æ¢è¡Œ
               else:
                   # éæµå¼è¾“å‡º
                   await self.process_message(user_input)
               
           except KeyboardInterrupt:
               print("\n\nğŸ‘‹ ä¼šè¯å·²ä¸­æ–­")
               break
           except Exception as e:
               print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
               continue
   
   async def _handle_command(self, command: str):
       """å¤„ç†å‘½ä»¤"""
       cmd_parts = command.split()
       cmd = cmd_parts[0].lower()
       
       if cmd == "/exit":
           print("ğŸ‘‹ å†è§ï¼")
           self.session_active = False
           
       elif cmd == "/reset":
           self.reset_session()
           
       elif cmd == "/mode":
           print("\né€‰æ‹©å·¥å…·è°ƒç”¨æ¨¡å¼:")
           print("1. never  - ä»ä¸è°ƒç”¨å·¥å…·")
           print("2. auto   - è‡ªåŠ¨å†³å®š(é»˜è®¤)")
           print("3. always - å§‹ç»ˆè°ƒç”¨å·¥å…·")
           
           choice = input("è¯·é€‰æ‹©(1/2/3): ").strip()
           mode_map = {"1": ToolMode.NEVER, "2": ToolMode.AUTO, "3": ToolMode.ALWAYS}
           
           if choice in mode_map:
               self.set_tool_mode(mode_map[choice])
           else:
               print("âš ï¸ æ— æ•ˆé€‰æ‹©")
               
       elif cmd == "/stream":
           current = self.llm.stream
           self.set_stream_mode(not current)
           
       elif cmd == "/max":
           if len(cmd_parts) > 1 and cmd_parts[1].isdigit():
               self.set_max_tool_calls(int(cmd_parts[1]))
           else:
               print("âš ï¸ è¯·æä¾›æœ‰æ•ˆæ•°å­—ï¼Œå¦‚: /max 5")
               
       elif cmd == "/history":
           print("\nğŸ“œ å¯¹è¯å†å²:")
           for i, msg in enumerate(self.llm.get_history(), 1):
               role = msg["role"]
               content = msg.get("content", "")
               
               if role == "tool":
                   # å·¥å…·ç»“æœç®€åŒ–æ˜¾ç¤º
                   print(f"{i}. [{role}]: <æœç´¢ç»“æœ...>")
               elif content:
                   # æ™®é€šæ¶ˆæ¯æˆªæ–­æ˜¾ç¤º
                   display = content[:100] + "..." if len(content) > 100 else content
                   print(f"{i}. [{role}]: {display}")
               elif msg.get("tool_calls"):
                   print(f"{i}. [{role}]: <è°ƒç”¨å·¥å…·>")
       else:
           print(f"âš ï¸ æœªçŸ¥å‘½ä»¤: {cmd}")

# ============================================
# FastAPIé€‚é…æ¥å£
# ============================================

class SessionManager:
   """ä¼šè¯ç®¡ç†å™¨ - ç”¨äºFastAPIé›†æˆ"""
   
   def __init__(self):
       self.sessions: Dict[str, WebSearchAgent] = {}
   
   def create_session(self, 
                      session_id: str,
                      api_key: str,
                      **kwargs) -> WebSearchAgent:
       """åˆ›å»ºæ–°ä¼šè¯"""
       # è½¬æ¢å·¥å…·æ¨¡å¼å­—ç¬¦ä¸²ä¸ºæšä¸¾
       if 'tool_mode' in kwargs and isinstance(kwargs['tool_mode'], str):
           kwargs['tool_mode'] = ToolMode(kwargs['tool_mode'])
           
       session = WebSearchAgent(api_key=api_key, **kwargs)
       self.sessions[session_id] = session
       return session
   
   def get_session(self, session_id: str) -> Optional[WebSearchAgent]:
       """è·å–ä¼šè¯"""
       return self.sessions.get(session_id)
   
   def delete_session(self, session_id: str) -> bool:
       """åˆ é™¤ä¼šè¯"""
       if session_id in self.sessions:
           del self.sessions[session_id]
           return True
       return False
   
   async def process_request_stream(self,
                                   session_id: str,
                                   message: str,
                                   create_if_not_exists: bool = True,
                                   api_key: Optional[str] = None,
                                   **session_kwargs) -> AsyncGenerator[Dict[str, Any], None]:
       """æµå¼å¤„ç†è¯·æ±‚ - è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ä¾›SSEä½¿ç”¨"""
       
       # è·å–æˆ–åˆ›å»ºä¼šè¯
       session = self.get_session(session_id)
       if not session and create_if_not_exists:
           if not api_key:
               yield {
                   "type": "error",
                   "data": "éœ€è¦æä¾›api_keyåˆ›å»ºæ–°ä¼šè¯"
               }
               return
           session = self.create_session(session_id, api_key, **session_kwargs)
       elif not session:
           yield {
               "type": "error",
               "data": "ä¼šè¯ä¸å­˜åœ¨"
           }
           return
       
       # æµå¼å¤„ç†æ¶ˆæ¯
       try:
           async for chunk in session.process_message_stream(message):
               yield chunk
       except Exception as e:
           yield {
               "type": "error",
               "data": str(e)
           }

# ============================================
# æµ‹è¯•å…¥å£
# ============================================

async def main():
   """æµ‹è¯•å…¥å£"""
   agent = WebSearchAgent(
       api_key="sk-f5889d58c6db4dd38ca78389a6c7a7e8",  
       tool_mode=ToolMode.AUTO,
       max_tool_calls=3,
       stream=True
   )
   
   await agent.run_interactive()

if __name__ == "__main__":
   asyncio.run(main())