# 0913 ä¼˜åŒ–å†å²æ¶ˆæ¯è®°å½•ç»“æ„ï¼šè§„åˆ’è§’è‰²å’Œå­—æ®µjson
# 0913 ä¼˜åŒ–ä¿¡æ¯ä¼ å…¥é€»è¾‘ï¼šæœ‰çŠ¶æ€å¯¹è¯ï¼šè®°å½•çŠ¶æ€â†’æ¸…ç©ºçŠ¶æ€
# 0913 å»æ‰_prepare_messageså’Œ_build_request_paramså†—æ‚æ–¹æ³•ï¼Œä½¿ç”¨add_messageç»Ÿä¸€ç®¡ç†æµå¼å’Œéæµå¼ï¼Œåœ¨æ›´é«˜å±‚ç»Ÿä¸€ç®¡ç†ç³»ç»Ÿæç¤ºè¯
# 0913 ä¼˜åŒ–é€’å½’é€»è¾‘ï¼Œä½¿ç”¨é€’å½’æ·±åº¦æ§åˆ¶å·¥å…·æ‰§è¡Œæ¬¡æ•°ï¼ˆæµå¼/éæµå¼ï¼‰



"""
é€šç”¨LLMç±» - æ”¯æŒDeepSeekç­‰OpenAIå…¼å®¹API
æ”¯æŒå¤šè½®å¯¹è¯ã€Function Callingã€æµå¼/éæµå¼è¾“å‡º
"""

from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionMessage
from typing import List, Dict, Optional, Literal, Callable, AsyncGenerator, Any, Union
import asyncio
import json

class LLM:
    """é€šç”¨å¤§æ¨¡å‹ç±»ï¼Œæ”¯æŒOpenAIå…¼å®¹çš„APIï¼ˆåŒ…æ‹¬DeepSeekï¼‰
    
    é‡æ„ä¼˜åŒ–ï¼š
    - ç»Ÿä¸€æ¶ˆæ¯å‡†å¤‡å’Œå‚æ•°æ„å»ºé€»è¾‘
    - æŠ½å–å·¥å…·æ‰§è¡Œå…¬å…±ä»£ç 
    - ç®€åŒ–æµå¼/éæµå¼å¤„ç†æµç¨‹
    """

    def __init__(self,
                 api_key: str,
                 base_url: str,
                 model: str = "deepseek-chat",
                 tool_choice: Literal["auto", "required", "none"] = "auto",
                 temperature: float = 1,
                 max_tokens: int = 4096):
        
        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.tool_choice = tool_choice
        self.conversation_history: List[Dict] = []

    # ============================================
    # åŸºç¡€æ–¹æ³•
    # ============================================
    # 0913 æ ‡å‡†åŒ–æ¶ˆæ¯å†å²ç»“æ„ï¼š4ä¸ªè§’è‰²ä»¥åŠæ¯ä¸ªè§’è‰²å†…éƒ¨å¯ä»¥å­˜åœ¨çš„å­—æ®µ
    def add_message(self, 
                    role: str, 
                    content: Optional[str] = None,
                    tool_calls: Optional[List[Dict]] = None,
                    tool_call_id: Optional[str] = None):
        """
        æ·»åŠ æ¶ˆæ¯åˆ°å¯¹è¯å†å²
        
        Args:
            role: è§’è‰² (system/user/assistant/tool)
            content: æ¶ˆæ¯å†…å®¹
            tool_calls: å·¥å…·è°ƒç”¨åˆ—è¡¨ (ä»…assistantè§’è‰²ä½¿ç”¨)
            tool_call_id: å·¥å…·è°ƒç”¨ID (ä»…toolè§’è‰²ä½¿ç”¨)
        """
        if role == "assistant":
            # Assistantæ¶ˆæ¯ï¼šcontentå§‹ç»ˆå­˜åœ¨ï¼ˆå¯ä¸ºNoneï¼‰ï¼Œtool_callså¯é€‰
            msg = {
                "role": "assistant",
                "content": content  # å¯ä»¥æ˜¯Noneæˆ–å­—ç¬¦ä¸²
            }
            if tool_calls:
                msg["tool_calls"] = tool_calls
                
        elif role == "tool":
            # Toolæ¶ˆæ¯ï¼šå¿…é¡»æœ‰contentå’Œtool_call_id
            if content is None or not tool_call_id:
                raise ValueError("Toolæ¶ˆæ¯å¿…é¡»åŒ…å«contentå’Œtool_call_id")
            msg = {
                "role": "tool",
                "content": content,
                "tool_call_id": tool_call_id
            }
            
        elif role in ["system", "user"]:
            # System/Useræ¶ˆæ¯ï¼šå¿…é¡»æœ‰content
            if content is None:
                raise ValueError(f"{role}æ¶ˆæ¯å¿…é¡»åŒ…å«content")
            msg = {
                "role": role, 
                "content": content
            }
        else:
            raise ValueError(f"æœªçŸ¥çš„è§’è‰²ç±»å‹: {role}")
        
        self.conversation_history.append(msg)
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []
    
    def get_history(self) -> List[Dict]:
        """è·å–å¯¹è¯å†å²"""
        return self.conversation_history.copy()

    # ============================================
    # å…¬å…±é€»è¾‘ - 1.è¯·æ±‚å‚æ•°æ„å»º 2.å·¥å…·æ‰§è¡Œ 
    # ============================================
      
    def _build_request_params(self,
                            tools: Optional[List[Dict]] = None,
                            temperature: Optional[float] = None,
                            max_tokens: Optional[int] = None,
                            tool_choice: Optional[Literal["auto", "required", "none"]] = None,
                            stream: bool = False) -> Dict:
        """æ„å»ºAPIè¯·æ±‚å‚æ•° - ç®€åŒ–ç‰ˆ"""
        
        params = {
            "model": self.model,
            "messages": self.conversation_history,  # ç›´æ¥ä½¿ç”¨å†å²
            "temperature": self.temperature if temperature is None else temperature,
            "max_tokens": self.max_tokens if max_tokens is None else max_tokens,
            "stream": stream
        }
        
        if tools:
            params["tools"] = tools
            params["tool_choice"] = self.tool_choice if tool_choice is None else tool_choice
        
        return params

    async def _execute_tool(self,
                          tool_call: Dict,
                          tool_functions: Dict[str, Callable]) -> tuple[str, str]:
        """æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨ - ç»Ÿä¸€å·¥å…·æ‰§è¡Œé€»è¾‘
        
        å¤„ç†assistantè§’è‰²è¿”å›çš„tool_callï¼Œæ‰§è¡Œå¯¹åº”çš„å·¥å…·å‡½æ•°å¹¶è¿”å›ç»“æœã€‚
        
        Args:
            tool_call (Dict): å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                {
                    "function": {
                        "name": str,        # è¦è°ƒç”¨çš„å‡½æ•°åç§°
                        "arguments": str    # JSONæ ¼å¼çš„å‚æ•°å­—ç¬¦ä¸²
                    }
                }
                
                ç¤ºä¾‹ï¼š
                {
                    "function": {
                        "name": "search_web",
                        "arguments": '{"query": "Pythonæ•™ç¨‹", "max_results": 5}'
                    }
                }
            
            tool_functions (Dict[str, Callable]): å¯ç”¨çš„å·¥å…·å‡½æ•°æ˜ å°„è¡¨
                é”®ä¸ºå‡½æ•°åç§°ï¼Œå€¼ä¸ºå¯¹åº”çš„å¯è°ƒç”¨å‡½æ•°å¯¹è±¡ã€‚
                
                æ ¼å¼ï¼š
                {
                    "å‡½æ•°åç§°": å‡½æ•°å¯¹è±¡,
                    ...
                }
                
                ç¤ºä¾‹ï¼š
                {
                    "search_web": search_web_func,
                    "calculate": calculate_func
                }
        
        Returns:
            tuple[str, str]: è¿”å›å…ƒç»„ (å‡½æ•°åç§°, æ‰§è¡Œç»“æœ)
                - å‡½æ•°åç§°: è¢«æ‰§è¡Œçš„å‡½æ•°å
                - æ‰§è¡Œç»“æœ: å‡½æ•°æ‰§è¡Œçš„ç»“æœå­—ç¬¦ä¸²ï¼Œæˆ–é”™è¯¯ä¿¡æ¯
        
        Note:
            - è‡ªåŠ¨å¤„ç†åŒæ­¥å’Œå¼‚æ­¥å‡½æ•°çš„æ‰§è¡Œ
            - å¦‚æœå‡½æ•°ä¸å­˜åœ¨ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æŠ›å‡ºå¼‚å¸¸
        """
        
        func_name = tool_call["function"]["name"]
        func_args = json.loads(tool_call["function"]["arguments"])
        
        if func_name in tool_functions:
            # åˆ¤æ–­æ˜¯å¦ä¸ºå¼‚æ­¥å‡½æ•°å¹¶æ‰§è¡Œ
            if asyncio.iscoroutinefunction(tool_functions[func_name]):
                result = await tool_functions[func_name](**func_args)
            else:
                result = tool_functions[func_name](**func_args)
            
            return func_name, str(result)
        else:
            error_msg = f"æœªæ‰¾åˆ°å·¥å…·: {func_name}"
            return func_name, error_msg
    
    # ============================================
    # æ ¸å¿ƒå¯¹è¯æ–¹æ³• - éæµå¼
    # ============================================
    
    async def chat_complete(self,
                        user_input: Optional[str] = None,
                        tools: Optional[List[Dict]] = None, # tool_schemas
                        tool_functions: Optional[Dict[str, Callable]] = None, # å·¥å…·æ˜ å°„è¡¨
                        temperature: Optional[float] = None,
                        max_tokens: Optional[int] = None,
                        tool_choice: Optional[Literal["auto", "required", "none"]] = None,
                        verbose: bool = False,
                        max_tool_rounds: int = 3,
                        _current_round: int = 0) -> str:
        """
        æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨å’Œæ€»ç»“å¼•å¯¼çš„å¯¹è¯æ–¹æ³•
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥ï¼ˆå¯é€‰ï¼‰
            tools: å·¥å…·schemaåˆ—è¡¨
            tool_functions: å·¥å…·å‡½æ•°æ˜ å°„
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokens
            tool_choice: å·¥å…·é€‰æ‹©æ¨¡å¼
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            max_tool_rounds: æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°ï¼ˆé»˜è®¤3è½®ï¼‰
            _current_round: å†…éƒ¨å‚æ•°ï¼Œå½“å‰é€’å½’è½®æ•°
            
        Returns:
            str: æ¨¡å‹çš„æ–‡æœ¬å›å¤
        """
        
        # åªåœ¨æœ‰ç”¨æˆ·è¾“å…¥æ—¶æ·»åŠ ï¼ˆç¬¬ä¸€è½®ï¼‰
        if user_input:
            self.add_message("user", user_input)
        
        # ç¡®ä¿æœ‰æ¶ˆæ¯å¯å‘é€
        if not self.conversation_history:
            raise ValueError("å¯¹è¯å†å²ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå¯¹è¯")
        
        # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ•°
        if _current_round >= max_tool_rounds:
            # è¾¾åˆ°ä¸Šé™ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆæ€»ç»“
            tool_choice = "none"
            
            # å†…éƒ¨é…ç½®çš„æ€»ç»“å¼•å¯¼æç¤º
            summary_prompt = (
                "åŸºäºä¸Šè¿°æ‰€æœ‰å·¥å…·è°ƒç”¨çš„ç»“æœï¼Œè¯·ç»¼åˆåˆ†æå¹¶ç”¨è‡ªç„¶ã€å‹å¥½çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
                "ç¡®ä¿ï¼š1) ç›´æ¥å›ç­”ç”¨æˆ·çš„åŸå§‹é—®é¢˜ 2) åŒ…å«æ‰€æœ‰ç›¸å…³ä¿¡æ¯ 3) è¯­è¨€ç®€æ´æ¸…æ™°ã€‚"
                "ä¸è¦æåŠå·¥å…·è°ƒç”¨çš„è¿‡ç¨‹ï¼Œç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚"
            )
            
            # æ·»åŠ æ€»ç»“å¼•å¯¼æç¤º
            self.add_message("user", summary_prompt)
            if verbose:
                print(f"[ç³»ç»Ÿ]: è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°({max_tool_rounds})ï¼Œæ·»åŠ æ€»ç»“å¼•å¯¼...")
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = self._build_request_params(
            tools=tools, 
            temperature=temperature, 
            max_tokens=max_tokens, 
            tool_choice=tool_choice, 
            stream=False
        )
        
        # APIè°ƒç”¨
        response = await self.client.chat.completions.create(**request_params)
        message = response.choices[0].message
        
        # æå–å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
        tool_calls = None
        # tools = [{å·¥å…·1},{å·¥å…·2ï¼ˆå¦‚æœ‰ï¼‰},...]
        if message.tool_calls:
            tool_calls = [
                {
                    "id": tc.id,
                    "type": tc.type,
                    "function": {
                        "name": tc.function.name,
                        "arguments": tc.function.arguments
                    }
                }
                for tc in message.tool_calls
            ]
        
        # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
        self.add_message("assistant", message.content, tool_calls)
        
        # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›ï¼ˆé€’å½’ç»ˆæ­¢æ¡ä»¶ï¼‰
        if not tool_calls or not tool_functions:
            if verbose:
                if message.content:
                    print(f"[åŠ©æ‰‹]: {message.content}")
                if _current_round > 0:
                    print(f"[ç³»ç»Ÿ]: å®Œæˆï¼Œå…±è¿›è¡Œäº† {_current_round} è½®å·¥å…·è°ƒç”¨")
            
            return message.content or ""
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        if verbose:
            print(f"[æ‰§è¡Œå·¥å…·è°ƒç”¨ - ç¬¬{_current_round + 1}è½®]:")
        
        for tool_call in message.tool_calls:
            # æ‰§è¡Œå·¥å…·
            func_name, result = await self._execute_tool(
                tool_call.model_dump(), # Pydantic â†’ ç”¨äºå°†æ¨¡å‹å®ä¾‹è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                tool_functions
            )
            
            if verbose:
                print(f"  - å·¥å…·: {func_name}")
                result_display = result[:100] + "..." if len(result) > 100 else result
                print(f"    ç»“æœ: {result_display}")
            
            # æ·»åŠ å·¥å…·ç»“æœåˆ°å†å²
            self.add_message("tool", content=result, tool_call_id=tool_call.id)
        
        # é€’å½’è°ƒç”¨ï¼Œå¢åŠ è½®æ•°è®¡æ•°
        final_response = await self.chat_complete(
            user_input=None,
            tools=tools,
            tool_functions=tool_functions,
            temperature=temperature,
            max_tokens=max_tokens,
            tool_choice=tool_choice,
            verbose=verbose,
            max_tool_rounds=max_tool_rounds,
            _current_round=_current_round + 1
        )
        
        return final_response
    
    # ============================================
    # æ ¸å¿ƒå¯¹è¯æ–¹æ³• - æµå¼
    # ============================================
    
    async def chat_stream(self,
                        user_input: Optional[str] = None,
                        tools: Optional[List[Dict]] = None,
                        tool_functions: Optional[Dict[str, Callable]] = None,
                        temperature: Optional[float] = None,
                        max_tokens: Optional[int] = None,
                        tool_choice: Optional[Literal["auto", "required", "none"]] = None,
                        max_tool_rounds: int = 3,
                        _current_round: int = 0) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼å¯¹è¯ - æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥ï¼ˆå¯é€‰ï¼‰
            tools: å·¥å…·schemaåˆ—è¡¨
            tool_functions: å·¥å…·å‡½æ•°æ˜ å°„
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokens
            tool_choice: å·¥å…·é€‰æ‹©æ¨¡å¼
            max_tool_rounds: æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°
            _current_round: å†…éƒ¨å‚æ•°ï¼Œå½“å‰é€’å½’è½®æ•°
            
        Yields:
            Dict: åŒ…å«typeå’Œdataçš„äº‹ä»¶å­—å…¸
        """
        
        # åªåœ¨æœ‰ç”¨æˆ·è¾“å…¥æ—¶æ·»åŠ ï¼ˆç¬¬ä¸€è½®ï¼‰
        if user_input:
            self.add_message("user", user_input)
        
        # ç¡®ä¿æœ‰æ¶ˆæ¯å¯å‘é€
        if not self.conversation_history:
            raise ValueError("å¯¹è¯å†å²ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå¯¹è¯")
        
        # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ•°
        if _current_round >= max_tool_rounds:
            tool_choice = "none"
            
            # æ·»åŠ æ€»ç»“å¼•å¯¼æç¤º
            summary_prompt = (
                "åŸºäºä¸Šè¿°æ‰€æœ‰å·¥å…·è°ƒç”¨çš„ç»“æœï¼Œè¯·ç»¼åˆåˆ†æå¹¶ç”¨è‡ªç„¶ã€å‹å¥½çš„è¯­è¨€å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
                "ç¡®ä¿ï¼š1) ç›´æ¥å›ç­”ç”¨æˆ·çš„åŸå§‹é—®é¢˜ 2) åŒ…å«æ‰€æœ‰ç›¸å…³ä¿¡æ¯ 3) è¯­è¨€ç®€æ´æ¸…æ™°ã€‚"
                "ä¸è¦æåŠå·¥å…·è°ƒç”¨çš„è¿‡ç¨‹ï¼Œç›´æ¥ç»™å‡ºç­”æ¡ˆã€‚"
            )
            self.add_message("user", summary_prompt)
            
            yield {"type": "system_info", "data": f"è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°({max_tool_rounds})ï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ..."}
        
        # æµå¼è°ƒç”¨æ ¸å¿ƒé€»è¾‘
        tool_calls_to_execute = []
        content_chunks = []
        
        async for chunk in self._stream_core(
            tools=tools, # tool_schemas
            temperature=temperature,
            max_tokens=max_tokens,
            tool_choice=tool_choice
        ):
            if chunk["type"] == "content":
                content_chunks.append(chunk["data"])
                yield chunk
            elif chunk["type"] == "tool_call_complete":
                tool_calls_to_execute.append(chunk["data"])
            elif chunk["type"] == "done":
                # åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œå·¥å…·
                if tool_calls_to_execute and tool_functions:
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    yield {"type": "tool_execution_start", "data": {"round": _current_round + 1}}
                    
                    for tool_call in tool_calls_to_execute:
                        func_name = tool_call["function"]["name"]
                        func_args = json.loads(tool_call["function"]["arguments"])
                        
                        yield {
                            "type": "tool_executing",
                            "data": {"name": func_name, "arguments": func_args}
                        }
                        
                        # æ‰§è¡Œå·¥å…·
                        _, result = await self._execute_tool(tool_call, tool_functions)
                        
                        # æ·»åŠ å·¥å…·ç»“æœåˆ°å†å²
                        self.add_message("tool", content=result, tool_call_id=tool_call["id"])
                        
                        yield {
                            "type": "tool_result",
                            "data": {"name": func_name, "result": result[:500]}
                        }
                    
                    # é€’å½’è°ƒç”¨ç»§ç»­ç”Ÿæˆ
                    yield {"type": "continue_generation", "data": {"round": _current_round + 1}}
                    
                    async for next_chunk in self.chat_stream(
                        user_input=None,  # ä¸å†æ·»åŠ ç”¨æˆ·è¾“å…¥
                        tools=tools,
                        tool_functions=tool_functions,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        tool_choice=tool_choice,
                        max_tool_rounds=max_tool_rounds,
                        _current_round=_current_round + 1
                    ):
                        yield next_chunk
                else:
                    # æ— å·¥å…·è°ƒç”¨ï¼Œæµå¼ç»“æŸ
                    yield chunk
    
    async def _stream_core(self,
                        tools: Optional[List[Dict]] = None,
                        temperature: Optional[float] = None,
                        max_tokens: Optional[int] = None,
                        tool_choice: Optional[Literal["auto", "required", "none"]] = None
                        ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼è°ƒç”¨æ ¸å¿ƒé€»è¾‘ - å¤„ç†å•æ¬¡æµå¼å“åº”
        ä¿®æ”¹ç‚¹ï¼š
        1. ç§»é™¤äº†messageså’Œuse_historyå‚æ•°
        2. ç›´æ¥ä½¿ç”¨self.conversation_history
        """
        
        # æ„å»ºè¯·æ±‚å‚æ•° - ç›´æ¥ä½¿ç”¨å†å²
        request_params = self._build_request_params(
            tools=tools,
            temperature=temperature,
            max_tokens=max_tokens,
            tool_choice=tool_choice,
            stream=True
        )
        
        # æµå¼è¯·æ±‚
        response = await self.client.chat.completions.create(**request_params)
        
        # æ”¶é›†å†…å®¹ç”¨äºå†å²è®°å½•
        collected_content = []
        collected_tool_calls = []
        current_tool_call = None
        
        # ç´¯è®¡æ–‡æœ¬å†…å®¹ + ç´¯è®¡å·¥å…·å‚æ•°
        async for chunk in response:
            delta = chunk.choices[0].delta
            
            # å¤„ç†æ–‡æœ¬å†…å®¹
            if delta.content:
                collected_content.append(delta.content)
                yield {"type": "content", "data": delta.content}
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            # æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if delta.tool_calls:
                for tool_call in delta.tool_calls:
                    # æ£€æŸ¥indexæ˜¯å¦å­˜åœ¨
                    if tool_call.index is not None:
                        # åˆ¤æ–­æ˜¯å¦æ˜¯æ–°å·¥å…·
                        if current_tool_call is None or tool_call.index != current_tool_call["index"]:
                            # æ˜¯æ–°å·¥å…·ï¼Œä¿å­˜ä¸Šä¸€ä¸ªå·¥å…·
                            if current_tool_call:
                                # ä¿å­˜ä¸Šä¸€ä¸ªå·¥å…·è°ƒç”¨ï¼ˆåªä¿ç•™æ ‡å‡†å­—æ®µï¼‰
                                standard_call = {
                                    "id": current_tool_call["id"],
                                    "type": current_tool_call["type"],
                                    "function": current_tool_call["function"]
                                }
                                collected_tool_calls.append(standard_call)
                                yield {"type": "tool_call_complete", "data": standard_call}
                            
                            current_tool_call = {
                                "id": tool_call.id or "",
                                "type": "function",
                                "index": tool_call.index,
                                "function": {"name": "", "arguments": ""}
                            }
                    
                    if tool_call.function and tool_call.function.name:
                        current_tool_call["function"]["name"] = tool_call.function.name
                    
                    if tool_call.function and tool_call.function.arguments:
                        current_tool_call["function"]["arguments"] += tool_call.function.arguments
                        yield {
                            "type": "tool_call_delta",
                            "data": {
                                "index": current_tool_call["index"],
                                "arguments_delta": tool_call.function.arguments
                            }
                        }
        
        # å¤„ç†æœ€åä¸€ä¸ªå·¥å…·è°ƒç”¨
        if current_tool_call:
            standard_call = {
                "id": current_tool_call["id"],
                "type": current_tool_call["type"],
                "function": current_tool_call["function"]
            }
            collected_tool_calls.append(standard_call)
            yield {"type": "tool_call_complete", "data": standard_call}
        
        # æ›´æ–°å†å² - ä½¿ç”¨add_messageç»Ÿä¸€å¤„ç†
        final_content = "".join(collected_content).strip() if collected_content else None
        
        if collected_tool_calls or final_content:
            self.add_message(
                "assistant",
                content=final_content,
                tool_calls=collected_tool_calls if collected_tool_calls else None
            )
        
        # å‘é€å®Œæˆä¿¡å·
        yield {
            "type": "done",
            "data": {
                "content": final_content,
                "tool_calls": collected_tool_calls if collected_tool_calls else None
            }
        }


# # ============================================
# # ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
# # ============================================

"""
LLMå¤šè½®å¯¹è¯æµ‹è¯•è„šæœ¬
æµ‹è¯•æµå¼å’Œéæµå¼åŠŸèƒ½ï¼Œå¹¶éªŒè¯æ¶ˆæ¯å†å²æ ¼å¼
"""

import asyncio
import json

async def test_multi_round_conversation():
    """æµ‹è¯•å¤šè½®å¯¹è¯å¹¶éªŒè¯æ¶ˆæ¯å†å²"""
    
    # åˆå§‹åŒ–LLM - ä¸å†ä¼ system_prompt
    llm = LLM(
        api_key="sk-f5889d58c6db4dd38ca78389a6c7a7e8",
        base_url="https://api.deepseek.com/v1",
        model="deepseek-chat",
        temperature=0.7
    )
    
    # æ‰‹åŠ¨æ·»åŠ ç³»ç»Ÿæç¤º
    llm.add_message("system", "ä½ æ˜¯ä¸€ä¸ªç®€æ´çš„AIåŠ©æ‰‹ï¼Œå›ç­”æ§åˆ¶åœ¨50å­—ä»¥å†…")
    
    # ç¬¬ä¸€è½®å¯¹è¯
    response = await llm.chat_complete(
        user_input="ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
        verbose=True
    )
    
    # ç¬¬äºŒè½®ï¼šç»§ç»­å¯¹è¯
    response = await llm.chat_complete(
        user_input="å®ƒçš„ä¸»è¦ä¼˜ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
        verbose=True
    )
    
    # æ‰“å°å¯¹è¯å†å²
    print("\nğŸ“ å½“å‰å¯¹è¯å†å²ï¼š")
    for i, msg in enumerate(llm.get_history(), 1):
        print(f"{i}. [{msg['role']}]: {msg.get('content', 'None')[:50]}...")
    
    print("\n" + "="*60)
    print("ğŸ”µ æµ‹è¯•2: æµå¼å¤šè½®å¯¹è¯")
    print("="*60)
    
    # æ¸…ç©ºå†å²ï¼Œå¼€å§‹æ–°å¯¹è¯
    llm.clear_history()
    
    # ç¬¬ä¸€è½®æµå¼å¯¹è¯
    print("[ç”¨æˆ·]: è®²ä¸ª10å­—çš„æ•…äº‹")
    print("[åŠ©æ‰‹]: ", end="")
    async for chunk in llm.chat_stream(
        user_input="è®²ä¸ª10å­—çš„æ•…äº‹"
    ):
        if chunk["type"] == "content":
            print(chunk["data"], end="", flush=True)
    print()
    
    # ç¬¬äºŒè½®æµå¼å¯¹è¯
    print("\n[ç”¨æˆ·]: å†è®²ä¸€ä¸ª")
    print("[åŠ©æ‰‹]: ", end="")
    async for chunk in llm.chat_stream(
        user_input="å†è®²ä¸€ä¸ª"
    ):
        if chunk["type"] == "content":
            print(chunk["data"], end="", flush=True)
    print()
    
    # æ‰“å°æµå¼å¯¹è¯å†å²
    print("\nğŸ“ æµå¼å¯¹è¯å†å²ï¼š")
    for i, msg in enumerate(llm.get_history(), 1):
        print(f"{i}. [{msg['role']}]: {msg.get('content', 'None')[:50]}...")

async def test_with_tools():
    """æµ‹è¯•å¸¦å·¥å…·çš„å¯¹è¯"""
    
    llm = LLM(
        api_key="sk-f5889d58c6db4dd38ca78389a6c7a7e8",
        base_url="https://api.deepseek.com/v1",
        model="deepseek-chat",
        temperature=0.7
    )
    
    print("\n" + "="*60)
    print("ğŸ”§ æµ‹è¯•3: å¸¦å·¥å…·çš„å¤šè½®å¯¹è¯")
    print("="*60)
    
    # å®šä¹‰ç®€å•å·¥å…·
    def get_time():
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime
        return datetime.now().strftime("%H:%M:%S")
    
    tools = [{
        "type": "function",
        "function": {
            "name": "get_time",
            "description": "è·å–å½“å‰æ—¶é—´",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": []
            }
        }
    }]
    
    tool_functions = {"get_time": get_time}
    
    # æ‰§è¡Œå·¥å…·è°ƒç”¨
    response = await llm.chat_complete(
        user_input="ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
        tools=tools,
        tool_functions=tool_functions,
        verbose=True
    )
    
    # ç»§ç»­å¯¹è¯ï¼ˆä¸ä½¿ç”¨å·¥å…·ï¼‰
    await llm.chat_complete(
        user_input="è°¢è°¢ï¼",
        verbose=True
    )
    
    # éªŒè¯æ¶ˆæ¯å†å²æ ¼å¼
    print("\nğŸ“ è¯¦ç»†æ¶ˆæ¯å†å²æ£€æŸ¥ï¼š")
    for i, msg in enumerate(llm.get_history(), 1):
        print(f"\næ¶ˆæ¯ {i}:")
        print(f"  role: {msg['role']}")
        print(f"  content: {msg.get('content', 'None')}")
        if 'tool_calls' in msg:
            print(f"  tool_calls: {json.dumps(msg['tool_calls'], indent=4)}")
        if 'tool_call_id' in msg:
            print(f"  tool_call_id: {msg['tool_call_id']}")

async def validate_message_format():
    """éªŒè¯æ¶ˆæ¯æ ¼å¼æ˜¯å¦ç¬¦åˆæ ‡å‡†"""
    
    print("\n" + "="*60)
    print("âœ… æ¶ˆæ¯æ ¼å¼éªŒè¯")
    print("="*60)
    
    llm = LLM(
        api_key="sk-f5889d58c6db4dd38ca78389a6c7a7e8",
        base_url="https://api.deepseek.com/v1",
        model="deepseek-chat"
    )
    
    # æµ‹è¯•å„ç§æ¶ˆæ¯ç±»å‹
    test_cases = [
        ("system", "ä½ æ˜¯åŠ©æ‰‹", None, None),
        ("user", "ä½ å¥½", None, None),
        ("assistant", "ä½ å¥½ï¼", None, None),
        ("assistant", None, [{"id": "call_123", "type": "function", 
                              "function": {"name": "test", "arguments": "{}"}}], None),
        ("tool", "ç»“æœ", None, "call_123")
    ]
    
    for role, content, tool_calls, tool_call_id in test_cases:
        try:
            llm.add_message(role, content, tool_calls, tool_call_id)
            print(f"âœ“ {role}æ¶ˆæ¯æ·»åŠ æˆåŠŸ")
        except Exception as e:
            print(f"âœ— {role}æ¶ˆæ¯å¤±è´¥: {e}")
    
    # æ˜¾ç¤ºæœ€ç»ˆå†å²
    print("\nğŸ“‹ æ ‡å‡†æ ¼å¼æ¶ˆæ¯å†å²ï¼š")
    print(llm.get_history())

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "ğŸš€ å¼€å§‹LLMå¤šè½®å¯¹è¯æµ‹è¯• ğŸš€".center(60, "="))
    
    try:
        # è¿è¡Œæµ‹è¯•
        await test_multi_round_conversation()
        await test_with_tools()
        await validate_message_format()
        
        print("\n" + "âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼".center(60, "="))
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
































# ============================================
# æ ‡å‡†çš„history_conversationç»“æ„
# ============================================
# # 1. Systemæ¶ˆæ¯
# {
#     "role": "system",
#     "content": "ç³»ç»ŸæŒ‡ä»¤æ–‡æœ¬"  # å¿…éœ€
# }

# # 2. Useræ¶ˆæ¯
# {
#     "role": "user",
#     "content": "ç”¨æˆ·è¾“å…¥æ–‡æœ¬"  # å¿…éœ€
# }

# # 3. Assistantæ¶ˆæ¯ - æ™®é€šå›å¤
# {
#     "role": "assistant",
#     "content": "åŠ©æ‰‹å›å¤æ–‡æœ¬"  # å¿…éœ€
# }

# # 4. Assistantæ¶ˆæ¯ - å·¥å…·è°ƒç”¨
# {
#     "role": "assistant",
#     "content": None,  # å¯ä»¥ä¸ºNoneæˆ–åŒ…å«æ–‡æœ¬
#     "tool_calls": [   # å·¥å…·è°ƒç”¨æ•°ç»„
#         {
#             "id": "å”¯ä¸€æ ‡è¯†ç¬¦",
#             "type": "function",
#             "function": {
#                 "name": "å‡½æ•°å",
#                 "arguments": "JSONå­—ç¬¦ä¸²æ ¼å¼çš„å‚æ•°"
#             }
#         }
#     ]
# }

# # 5. Toolæ¶ˆæ¯
# {
#     "role": "tool",
#     "content": "å·¥å…·æ‰§è¡Œç»“æœæ–‡æœ¬",  # å¿…éœ€
#     "tool_call_id": "å¯¹åº”çš„è°ƒç”¨id"  # å¿…éœ€ï¼Œå…³è”åˆ°assistantçš„tool_calls
# }