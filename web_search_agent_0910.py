"""Webæœç´¢Agentä¼šè¯å±‚ - é€‚é…LLMç±»çš„ä¼˜åŒ–ç‰ˆ"""

import asyncio
import json
from typing import Optional, Dict, List, Literal, Any, AsyncGenerator
from llm_0908 import LLM
from tools.tavily_search import tavily_search
from tools.function_schema import tools

class WebSearchAgent:
    """Webæœç´¢Agentä¼šè¯ç®¡ç†ç±»"""
    
    def __init__(self,
                 api_key: str,
                 base_url: str = "https://api.deepseek.com/v1",
                 model: str = "deepseek-chat",
                 tool_choice: Literal["auto", "required", "none"] = "auto",
                 max_tool_calls: int = 3,
                 temperature: float = 0.7,
                 max_tokens: int = 4096,
                 stream: bool = True):
        """åˆå§‹åŒ–WebSearchAgent"""
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.tool_choice = tool_choice
        self.max_tool_calls = max_tool_calls
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.stream = stream
        
        # åˆå§‹åŒ–LLMå®ä¾‹
        self._init_llm()
        
        # å·¥å…·å‡½æ•°æ˜ å°„
        self.tool_functions = {
            "tavily_search": tavily_search
        }
        
        self.session_active = True
        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥é€šè¿‡æœç´¢å·¥å…·è·å–æœ€æ–°çš„äº’è”ç½‘ä¿¡æ¯ã€‚

æœç´¢ç­–ç•¥æŒ‡å¯¼ï¼š
1. å½“ç”¨æˆ·è¯¢é—®éœ€è¦å®æ—¶ä¿¡æ¯çš„é—®é¢˜æ—¶ï¼Œä½¿ç”¨æœç´¢å·¥å…·
2. å¦‚æœé¦–æ¬¡æœç´¢ç»“æœä¸å¤Ÿå……åˆ†æˆ–ç›¸å…³ï¼Œå¯ä»¥è¿›è¡Œå¤šè½®æœç´¢ï¼š
   - å°è¯•ä¸åŒçš„æœç´¢å…³é”®è¯
   - ä½¿ç”¨æ›´å…·ä½“æˆ–æ›´å®½æ³›çš„æœç´¢è¯
   - åŸºäºå‰æ¬¡æœç´¢ç»“æœè°ƒæ•´æœç´¢ç­–ç•¥
3. æœ€å¤šå¯ä»¥è¿›è¡Œ3æ¬¡æœç´¢ï¼Œè¯·åˆç†åˆ©ç”¨æœç´¢æ¬¡æ•°
4. åŸºäºæ‰€æœ‰æœç´¢ç»“æœæä¾›å‡†ç¡®ã€å…¨é¢ã€æœ‰å¸®åŠ©çš„å›ç­”

æœç´¢è´¨é‡è¯„ä¼°ï¼š
- å¦‚æœæœç´¢ç»“æœä¸ç”¨æˆ·é—®é¢˜é«˜åº¦ç›¸å…³ä¸”ä¿¡æ¯å……åˆ†ï¼Œå¯ä»¥åœæ­¢æœç´¢
- å¦‚æœæœç´¢ç»“æœä¸å¤Ÿç›¸å…³æˆ–ä¿¡æ¯ä¸è¶³ï¼Œç»§ç»­ä¼˜åŒ–æœç´¢ç­–ç•¥"""
    
    def _init_llm(self):
        """åˆå§‹åŒ–æˆ–é‡æ–°åˆå§‹åŒ–LLMå®ä¾‹"""
        self.llm = LLM(
            api_key=self.api_key,
            base_url=self.base_url,
            model=self.model,
            tool_choice=self.tool_choice,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            stream=self.stream
        )
        # æ³¨å…¥ç³»ç»Ÿæç¤º
        self.llm.add_message("system", self.system_prompt)
        self.tool_call_count = 0
    
    def reset_session(self):
        """é‡ç½®ä¼šè¯çŠ¶æ€"""
        self._init_llm()
        print("âœ¨ ä¼šè¯å·²é‡ç½®\n")
    
    def set_tool_choice(self, choice: Literal["auto", "required", "none"]):
        """è®¾ç½®å·¥å…·é€‰æ‹©æ¨¡å¼"""
        self.tool_choice = choice
        self.llm.tool_choice = choice
        print(f"ğŸ”§ å·¥å…·é€‰æ‹©æ¨¡å¼è®¾ç½®ä¸º: {choice}\n")
    
    def set_max_tool_calls(self, max_calls: int):
        """è®¾ç½®æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°"""
        self.max_tool_calls = max_calls
        print(f"ğŸ“Š æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°è®¾ç½®ä¸º: {max_calls}\n")
    
    def set_stream_mode(self, stream: bool):
        """è®¾ç½®æµå¼è¾“å‡ºæ¨¡å¼"""
        self.stream = stream
        self.llm.stream = stream
        mode_str = "æµå¼" if stream else "éæµå¼"
        print(f"ğŸ“¡ è¾“å‡ºæ¨¡å¼è®¾ç½®ä¸º: {mode_str}\n")
    
    async def process_message_stream(self, user_input: str) -> AsyncGenerator[Dict[str, Any], None]:
        """æµå¼å¤„ç†å•æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆé€‚é…LLMç±»ï¼‰"""
        self.tool_call_count = 0
        
        # åˆ›å»ºå—é™çš„å·¥å…·å‡½æ•°æ˜ å°„
        limited_tool_functions = {}
        tool_limit_reached = False
        
        for name, func in self.tool_functions.items():
            async def limited_wrapper(*args, _original_func=func, _name=name, **kwargs):
                nonlocal tool_limit_reached
                
                # å¢åŠ è®¡æ•°
                self.tool_call_count += 1
                
                # æ‰§è¡ŒåŸå‡½æ•°
                if asyncio.iscoroutinefunction(_original_func):
                    result = await _original_func(*args, **kwargs)
                else:
                    result = _original_func(*args, **kwargs)
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é™åˆ¶
                if self.tool_call_count >= self.max_tool_calls:
                    tool_limit_reached = True
                    result += f"\n\n[ç³»ç»Ÿæç¤ºï¼šå·²è¾¾åˆ°æœ€å¤§æœç´¢æ¬¡æ•°{self.max_tool_calls}æ¬¡ï¼Œè¯·åŸºäºç°æœ‰ä¿¡æ¯ç»™å‡ºç»¼åˆå›ç­”]"
                
                return result
            
            limited_tool_functions[name] = limited_wrapper
        
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šå¸¦å·¥å…·çš„æµå¼å¤„ç†
            tools_to_use = None if self.tool_choice == "none" else tools
            
            async for chunk in self.llm.chat_with_tools_stream(
                user_input=user_input,
                tools=tools_to_use,
                tool_functions=limited_tool_functions
            ):
                yield chunk
                
                # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°å·¥å…·è°ƒç”¨é™åˆ¶
                if chunk["type"] == "tool_result" and tool_limit_reached:
                    # é€šçŸ¥å‰ç«¯è¾¾åˆ°é™åˆ¶
                    yield {
                        "type": "tool_limit",
                        "data": f"å·²è¾¾åˆ°æœ€å¤§è°ƒç”¨æ¬¡æ•°({self.max_tool_calls}æ¬¡)"
                    }
                
                # å¦‚æœæµç»“æŸä¸”è¾¾åˆ°é™åˆ¶ï¼Œéœ€è¦ç”Ÿæˆæœ€ç»ˆæ€»ç»“
                if chunk["type"] == "done" and tool_limit_reached:
                    # ç¬¬äºŒé˜¶æ®µï¼šç”Ÿæˆæœ€ç»ˆæ€»ç»“ï¼ˆæ— å·¥å…·ï¼‰
                    yield {"type": "final_summary_start", "data": "æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ..."}
                    
                    # ä¸´æ—¶ç¦ç”¨å·¥å…·
                    original_tool_choice = self.llm.tool_choice
                    self.llm.tool_choice = "none"
                    
                    try:
                        # æ·»åŠ æ€»ç»“æç¤º
                        self.llm.add_message(
                            "system", 
                            "è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰æœç´¢ç»“æœï¼Œç»™å‡ºå…¨é¢ã€å‡†ç¡®çš„æœ€ç»ˆå›ç­”ã€‚"
                        )
                        
                        # ç”Ÿæˆæœ€ç»ˆæ€»ç»“
                        async for summary_chunk in self.llm.chat_stream():
                            if summary_chunk["type"] == "content":
                                yield {
                                    "type": "final_summary",
                                    "data": summary_chunk["data"]
                                }
                            elif summary_chunk["type"] == "done":
                                yield summary_chunk
                                break
                    finally:
                        # æ¢å¤åŸå§‹è®¾ç½®
                        self.llm.tool_choice = original_tool_choice
                        # ç§»é™¤ä¸´æ—¶çš„ç³»ç»Ÿæ¶ˆæ¯
                        if self.llm.conversation_history[-2]["role"] == "system":
                            self.llm.conversation_history.pop(-2)
                        
        except Exception as e:
            yield {"type": "error", "data": f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}"}
    
    async def process_message(self, user_input: str) -> str:
        """éæµå¼å¤„ç†å•æ¡ç”¨æˆ·æ¶ˆæ¯ï¼ˆé€‚é…LLMç±»ï¼‰"""
        self.tool_call_count = 0
        tools_to_use = None if self.tool_choice == "none" else tools
        
        try:
            # åˆ›å»ºå—é™çš„å·¥å…·å‡½æ•°æ˜ å°„
            limited_tool_functions = {}
            tool_limit_reached = False
            
            for name, func in self.tool_functions.items():
                async def limited_wrapper(*args, _original_func=func, _name=name, **kwargs):
                    nonlocal tool_limit_reached
                    
                    self.tool_call_count += 1
                    
                    if asyncio.iscoroutinefunction(_original_func):
                        result = await _original_func(*args, **kwargs)
                    else:
                        result = _original_func(*args, **kwargs)
                    
                    if self.tool_call_count >= self.max_tool_calls:
                        tool_limit_reached = True
                        result += f"\n\n[ç³»ç»Ÿæç¤ºï¼šå·²è¾¾åˆ°æœ€å¤§æœç´¢æ¬¡æ•°{self.max_tool_calls}æ¬¡]"
                    
                    return result
                
                limited_tool_functions[name] = limited_wrapper
            
            # ç¬¬ä¸€é˜¶æ®µï¼šæ‰§è¡Œå¸¦å·¥å…·çš„å¯¹è¯
            result = await self.llm.chat_with_tools(
                user_input=user_input,
                tools=tools_to_use,
                tool_functions=limited_tool_functions,
                stream=False
            )
            
            # ç¬¬äºŒé˜¶æ®µï¼šå¦‚æœè¾¾åˆ°é™åˆ¶ï¼Œç”Ÿæˆæœ€ç»ˆæ€»ç»“
            if tool_limit_reached:
                print("\n[ç”Ÿæˆæœ€ç»ˆæ€»ç»“]:")
                
                # ä¸´æ—¶ç¦ç”¨å·¥å…·
                original_tool_choice = self.llm.tool_choice
                self.llm.tool_choice = "none"
                
                try:
                    # æ·»åŠ æ€»ç»“æç¤ºå¹¶ç”Ÿæˆæœ€ç»ˆå›ç­”
                    self.llm.add_message(
                        "user",
                        "è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰æœç´¢ç»“æœï¼Œç»™å‡ºå…¨é¢ã€å‡†ç¡®çš„æœ€ç»ˆå›ç­”ã€‚"
                    )
                    
                    final_response = await self.llm.chat(
                        tools=None,
                        stream=False
                    )
                    
                    final_result = final_response.content
                    print(final_result)
                    
                    # æ¸…ç†ä¸´æ—¶æ·»åŠ çš„æ¶ˆæ¯
                    self.llm.conversation_history.pop()  # ç§»é™¤ä¸´æ—¶useræ¶ˆæ¯
                    self.llm.conversation_history.pop()  # ç§»é™¤ç”Ÿæˆçš„assistantæ¶ˆæ¯
                    
                    # å°†æœ€ç»ˆç»“æœä½œä¸ºåŸå§‹è¯·æ±‚çš„å›å¤æ·»åŠ åˆ°å†å²
                    self.llm.conversation_history[-1] = {
                        "role": "assistant",
                        "content": final_result
                    }
                    
                    return final_result
                    
                finally:
                    self.llm.tool_choice = original_tool_choice
            
            return result
            
        except Exception as e:
            return f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}"

# ============================================
# æµ‹è¯•å…¥å£
# ============================================

async def test_stream():
    """æµ‹è¯•æµå¼è¾“å‡º"""
    agent = WebSearchAgent(
        api_key="sk-f5889d58c6db4dd38ca78389a6c7a7e8",
        tool_choice="auto",
        max_tool_calls=2,  # è®¾ç½®è¾ƒå°çš„å€¼ä¾¿äºæµ‹è¯•
        stream=True
    )
    
    print("=== æµå¼æµ‹è¯• ===\n")
    test_queries = [
        "ä»Šå¤©åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "æœ€æ–°çš„AIå‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿç»™æˆ‘è¯¦ç»†åˆ†æä¸€ä¸‹å„ä¸ªæ–¹é¢ã€‚"  # è¿™ä¸ªä¼šè§¦å‘å¤šæ¬¡æœç´¢
    ]
    
    for query in test_queries:
        print(f"\nç”¨æˆ·: {query}\n")
        print("åŠ©æ‰‹: ", end="")
        
        async for chunk in agent.process_message_stream(query):
            if chunk["type"] == "content":
                print(chunk["data"], end="", flush=True)
            elif chunk["type"] == "tool_executing":
                print(f"\nğŸ”§ æ‰§è¡Œå·¥å…·: {chunk['data']['name']}")
                print(f"   å‚æ•°: {chunk['data']['arguments']}")
            elif chunk["type"] == "tool_result":
                print(f"   ç»“æœé¢„è§ˆ: {chunk['data']['result'][:100]}...")
                print("\nç»§ç»­ç”Ÿæˆ: ", end="")
            elif chunk["type"] == "tool_limit":
                print(f"\nâš ï¸ {chunk['data']}")
            elif chunk["type"] == "final_summary_start":
                print(f"\nğŸ“ {chunk['data']}\næœ€ç»ˆç­”æ¡ˆ: ", end="")
            elif chunk["type"] == "final_summary":
                print(chunk["data"], end="", flush=True)
        
        print("\n" + "="*60)
        
        # é‡ç½®ä¼šè¯ä»¥æµ‹è¯•ä¸‹ä¸€ä¸ªæŸ¥è¯¢
        agent.reset_session()

async def test_non_stream():
    """æµ‹è¯•éæµå¼è¾“å‡º"""
    agent = WebSearchAgent(
        api_key="sk-f5889d58c6db4dd38ca78389a6c7a7e8",
        tool_choice="auto",
        max_tool_calls=2,
        stream=False
    )
    
    print("\n=== éæµå¼æµ‹è¯• ===\n")
    result = await agent.process_message("åˆ†æä¸€ä¸‹2024å¹´çš„å…¨çƒç»æµå½¢åŠ¿")
    print(f"\næœ€ç»ˆç»“æœé•¿åº¦: {len(result)} å­—ç¬¦")

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    await test_stream()
    await test_non_stream()

if __name__ == "__main__":
    asyncio.run(main())